Hereâ€™s a complete, well-structured `README.md` tailored for your project, assuming it's located in a GitHub repo and follows best practices:

---

```markdown
# Knowledge Distillation for Neural Collaborative Filtering (NeuMF)

This project explores the effectiveness of **Knowledge Distillation** techniques in reducing the parameter size of the [NeuMF](https://arxiv.org/abs/1708.05031) model without significant loss in recommendation performance. It is based on the **MovieLens 100K** dataset and was completed as part of the university course **"Deep Learning and Its Applications"**.

## ğŸ” Objectives

- Reproduce NeuMF results on a different dataset (MovieLens 100K).
- Investigate the impact of architecture choices (e.g. number of MLP layers, embedding size).
- Compare NeuMF to NMF in terms of HR@10, NDCG@10, and model size.
- Implement and evaluate 3 different **knowledge distillation** strategies:
  - Response-based distillation
  - Feature-based distillation
  - Relation-based distillation

## ğŸ“ Repository Structure

```

.
â”œâ”€â”€ ex\_12.py                 # Main training/evaluation script for student distillation
â”œâ”€â”€ Dataset.py               # Dataset loader and preprocessor
â”œâ”€â”€ evaluate.py              # Evaluation metrics (HR\@K, NDCG\@K)
â”œâ”€â”€ NeuMF.py                 # NeuMF model definition
â”œâ”€â”€ MLP.py                   # Standalone MLP (student model)
â”œâ”€â”€ Pretrain/                # Folder with pre-trained teacher weights (.npy)
â”œâ”€â”€ logs\*/                   # Output logs from distillation experiments
â”œâ”€â”€ script\_12.sh             # Shell script to automate 10-run experiments
â””â”€â”€ Data/
â””â”€â”€ ml-100k/             # MovieLens 100K data files

````

## âš™ï¸ Installation

Create a conda environment (recommended):
```bash
conda create -n ncf python=3.6 tensorflow=1.14 numpy pandas
conda activate ncf
````

Install additional dependencies:

```bash
pip install scikit-learn matplotlib
```


## ğŸ“Œ Notes

* All experiments are repeated **10 times**, and results are reported as **mean Â± std**.
* Negative sampling (`--num_neg`) and architecture settings are configurable.
* Pretraining is optional; this project focuses on *training from scratch* + distillation.

## ğŸ“š References

* Xiangnan He et al. "[Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031)", WWW 2017.
* Hinton et al. "[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)", NIPS 2015.
* Knowledge Distillation Benchmark: [https://arxiv.org/abs/2006.05525](https://arxiv.org/abs/2006.05525)

## ğŸ‘¤ Authors
* **\Kleidonaris Dimitris** â€“ MSc Student, Department of Electrical & Computer Engineering, University of Thessaly
* **\Aggelos Tzikas** â€“ MSc Student, Department of Electrical & Computer Engineering, University of Thessaly

## ğŸ“… Submission

